{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "13f899f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python \n",
    "import os\n",
    "import warnings\n",
    "import logging\n",
    "from typing import Mapping, List\n",
    "from pprint import pprint\n",
    "import plotly.express as px\n",
    "\n",
    "# Numpy and Pandas \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# PyTorch \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Transformers \n",
    "from transformers import AutoTokenizer, AutoModel, TrainingArguments, Trainer, AutoConfig\n",
    "\n",
    "# Catalyst\n",
    "from catalyst.dl import SupervisedRunner, Runner\n",
    "from catalyst.callbacks import AccuracyCallback, AUCCallback, OptimizerCallback\n",
    "from catalyst.callbacks import CheckpointCallback\n",
    "from catalyst.utils import set_global_seed, prepare_cudnn, load_checkpoint, unpack_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "c2e52704",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "38c9b47a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.18.0'"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "a491c2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "0c880784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to reproduce, download the data and customize this path\n",
    "PATH_TO_MODEL = '/home/timur/Desktop/jupyter_wf/bert-finetuning-catalyst/logdir/model.best.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "50321580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to reproduce, download the data and customize this path\n",
    "PATH_TO_DATA = '/home/timur/Desktop/jupyter_wf/contacts-in-item-TimuJ/model_notebook/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "0ae84f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(PATH_TO_DATA + 'train.csv').fillna('')\n",
    "valid_df = pd.read_csv(PATH_TO_DATA + 'valid.csv').fillna('')\n",
    "test_df = pd.read_csv(PATH_TO_DATA + 'test.csv').fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "765135a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>category</th>\n",
       "      <th>price</th>\n",
       "      <th>region</th>\n",
       "      <th>city</th>\n",
       "      <th>datetime_submitted</th>\n",
       "      <th>is_bad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11365</td>\n",
       "      <td>AirPods</td>\n",
       "      <td>Airpods /\\nAirPods 2 C–êMA–Ø –õ–£–ß–®A–Ø –∏ –¢O–ß–ù–ê–Ø –öO–ü–ò–Ø –æ—Ä–∏–≥–∏–Ω–∞–ª–∞ - 5:5, –ü–ûC–õE–î–ù–Ø–Ø M–û–î–ï–õ–¨ - –æ–∫—Ç—è–±—Ä—å 205...</td>\n",
       "      <td>–ê—É–¥–∏–æ –∏ –≤–∏–¥–µ–æ</td>\n",
       "      <td>–ë—ã—Ç–æ–≤–∞—è —ç–ª–µ–∫—Ç—Ä–æ–Ω–∏–∫–∞</td>\n",
       "      <td>3489.0</td>\n",
       "      <td>–†–æ—Å—Å–∏—è</td>\n",
       "      <td>–ú–æ—Å–∫–≤–∞</td>\n",
       "      <td>2019-10-13 12:43:42.299084</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11366</td>\n",
       "      <td>–ú–µ—Ö–∞–Ω–∏–∑–º —Å—Ç–µ–∫–ª–æ–æ—á–∏—Å—Ç–∏—Ç–µ–ª—è –ø–µ—Ä–µ–¥–Ω–∏–π Nissan Serena 4</td>\n",
       "      <td>–ú–µ—Ö–∞–Ω–∏–∑–º —Å—Ç–µ–∫–ª–æ–æ—á–∏—Å—Ç–∏—Ç–µ–ª—è Nissan Serena 1 C62M GA13DE 1991 –ø–µ—Ä–µ–¥–Ω–∏–π (–±/—É)/\\n/\\n –ú–∞—Ä–∫–∞: Nissan/\\n...</td>\n",
       "      <td>–ó–∞–ø—á–∞—Å—Ç–∏ –∏ –∞–∫—Å–µ—Å—Å—É–∞—Ä—ã</td>\n",
       "      <td>–¢—Ä–∞–Ω—Å–ø–æ—Ä—Ç</td>\n",
       "      <td>2500.0</td>\n",
       "      <td>–õ–∏–ø–µ—Ü–∫–∞—è –æ–±–ª–∞—Å—Ç—å</td>\n",
       "      <td>–õ–∏–ø–µ—Ü–∫</td>\n",
       "      <td>2019-10-13 12:45:06.628388</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11367</td>\n",
       "      <td>–ù—à-42–≤-4–ª</td>\n",
       "      <td>–Ω–æ–≤–æ–µ. —Ç–µ–ª 89024003360</td>\n",
       "      <td>–ó–∞–ø—á–∞—Å—Ç–∏ –∏ –∞–∫—Å–µ—Å—Å—É–∞—Ä—ã</td>\n",
       "      <td>–¢—Ä–∞–Ω—Å–ø–æ—Ä—Ç</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>–ß–µ–ª—è–±–∏–Ω—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å</td>\n",
       "      <td>–ß–µ–ª—è–±–∏–Ω—Å–∫</td>\n",
       "      <td>2019-10-13 12:45:44.957825</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11368</td>\n",
       "      <td>–®–∏–Ω—ã</td>\n",
       "      <td>–î–≤–∞ —Å–∫–∞—Ç–∞ DUNLOP –Ø–ø–æ–Ω–∏—è 683 -53-63 —Å–æ—Å—Ç–æ—è–Ω–∏–µ –Ω–æ–≤—ã—Ö –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏—Å—å –æ–¥–∏–Ω —Å–µ–∑–æ–Ω 1 –º–µ—Å—è—Ü–∞ —Ü–µ–Ω–∞ –∑–∞ 2 ...</td>\n",
       "      <td>–ó–∞–ø—á–∞—Å—Ç–∏ –∏ –∞–∫—Å–µ—Å—Å—É–∞—Ä—ã</td>\n",
       "      <td>–¢—Ä–∞–Ω—Å–ø–æ—Ä—Ç</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>–†–æ—Å—Ç–æ–≤—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å</td>\n",
       "      <td>–ü—Ä–æ–ª–µ—Ç–∞—Ä—Å–∫</td>\n",
       "      <td>2019-10-13 12:45:45.456624</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11369</td>\n",
       "      <td>–ñ–∏–≤—ã–µ —Ä–∞–∫–∏</td>\n",
       "      <td>–í –ø—Ä–æ–¥–∞–∂–µ —Ä–∞–∫ –¥–∏–∫–∏–π, —Ä–µ—á–Ω–æ–π./\\n–û–ø—Ç–æ–º –æ—Ç 50 –∫–≥, –≤ —Ä–æ–∑–Ω–∏—Ü—É –æ—Ç 5 –∫–≥./\\n–¶–µ–Ω–∞ –æ–ø—Ç / —Ä–æ–∑–Ω–∏—Ü–∞:/\\n/\\n–º–µ–ª...</td>\n",
       "      <td>–ü—Ä–æ–¥—É–∫—Ç—ã –ø–∏—Ç–∞–Ω–∏—è</td>\n",
       "      <td>–î–ª—è –¥–æ–º–∞ –∏ –¥–∞—á–∏</td>\n",
       "      <td>400.0</td>\n",
       "      <td>–†–æ—Å—Å–∏—è</td>\n",
       "      <td>–ú–æ—Å–∫–≤–∞</td>\n",
       "      <td>2019-10-13 12:48:10.767700</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4867</th>\n",
       "      <td>16232</td>\n",
       "      <td>–û—Ñ–∏—Å–Ω–æ–µ –ø–æ–º–µ—â–µ–Ω–∏–µ</td>\n",
       "      <td>–°–¥–∞—é—Ç—Å—è –æ—Ñ–∏—Å–Ω–æ–µ –ø–æ–º–µ—â–µ–Ω–∏–µ –ø–æ –£–ª –ö–∞–ª–∏–Ω–∏–Ω–∞ 80. –ü–æ–º–µ—â–µ–Ω–∏—è –Ω–∞—Ö–æ–¥—è—Ç—Å—è –≤ –æ—Ñ–∏—Å–Ω–æ–º —Ü–µ–Ω—Ç—Ä–µ. –í —Å—Ç–æ–∏–º–æ—Å—Ç—å –∞...</td>\n",
       "      <td>–ö–æ–º–º–µ—Ä—á–µ—Å–∫–∞—è –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç—å</td>\n",
       "      <td>–ù–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç—å</td>\n",
       "      <td>9200.0</td>\n",
       "      <td>–ß—É–≤–∞—à–∏—è</td>\n",
       "      <td>–ß–µ–±–æ–∫—Å–∞—Ä—ã</td>\n",
       "      <td>2019-10-14 23:57:30.094904</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4868</th>\n",
       "      <td>16233</td>\n",
       "      <td>iPhone 8 Plus Silver 25GB</td>\n",
       "      <td>–û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ä–æ—Å—Ç–µ—Å—Ç –∞–π—Ñ–æ–Ω. –ò–¥–µ–∞–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ, –Ω–∏ —Ä–∞–∑—É –Ω–µ —Ä–µ–º–æ–Ω—Ç–∏—Ä–æ–≤–∞–ª—Å—è –∏ –Ω–µ —Ä–∞–∑–±–∏—Ä–∞–ª—Å—è. /\\n–ö...</td>\n",
       "      <td>–¢–µ–ª–µ—Ñ–æ–Ω—ã</td>\n",
       "      <td>–ë—ã—Ç–æ–≤–∞—è —ç–ª–µ–∫—Ç—Ä–æ–Ω–∏–∫–∞</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>–¢–∞—Ç–∞—Ä—Å—Ç–∞–Ω</td>\n",
       "      <td>–ö–∞–∑–∞–Ω—å</td>\n",
       "      <td>2019-10-14 23:57:50.610616</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4869</th>\n",
       "      <td>16234</td>\n",
       "      <td>6-–∫ –∫–≤–∞—Ä—Ç–∏—Ä–∞, 54 –º¬≤, 5/9 —ç—Ç.</td>\n",
       "      <td>_________________________________________________________/\\n /\\n–ü–†–û–°–¢–û–†–ù–ê–Ø –ö–í–ê–†–¢–ò–†–ê –° –•–û–†–û–®–ï–ô –ü–õ...</td>\n",
       "      <td>–ö–≤–∞—Ä—Ç–∏—Ä—ã</td>\n",
       "      <td>–ù–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç—å</td>\n",
       "      <td>3300000.0</td>\n",
       "      <td>–°—Ç–∞–≤—Ä–æ–ø–æ–ª—å—Å–∫–∏–π –∫—Ä–∞–π</td>\n",
       "      <td>–ü—è—Ç–∏–≥–æ—Ä—Å–∫</td>\n",
       "      <td>2019-10-14 23:58:02.781579</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4870</th>\n",
       "      <td>16235</td>\n",
       "      <td>4-–∫ –∫–≤–∞—Ä—Ç–∏—Ä–∞, 60 –º¬≤, 9/9 —ç—Ç.</td>\n",
       "      <td>/\\n /\\n‚óè –†–∞–±–æ—Ç–∞–µ–º –ë–ï–ó –ü–ï–†–ï–†–´–í–û–í –ò –í–´–•–û–î–ù–´–• —Å 9:00 –¥–æ 12:00./\\n /\\n‚óè –û–±–º–µ–Ω—è–µ–º –∏ –ø—Ä–æ–¥–∞–¥–∏–º –í–∞—à—É –Ω–µ...</td>\n",
       "      <td>–ö–≤–∞—Ä—Ç–∏—Ä—ã</td>\n",
       "      <td>–ù–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç—å</td>\n",
       "      <td>2300000.0</td>\n",
       "      <td>–°—Ç–∞–≤—Ä–æ–ø–æ–ª—å—Å–∫–∏–π –∫—Ä–∞–π</td>\n",
       "      <td>–ü—è—Ç–∏–≥–æ—Ä—Å–∫</td>\n",
       "      <td>2019-10-14 23:59:01.435691</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4871</th>\n",
       "      <td>16236</td>\n",
       "      <td>BMW 7 —Å–µ—Ä–∏—è, 1031</td>\n",
       "      <td>–ú–∞—à–∏–Ω–∞ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏ –≤ –æ—Ç–ª–∏—á–Ω–æ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ –≤—Å–µ —Å–∏—Å—Ç–µ–º—ã —Ä–∞–±–æ—Ç–∞—é—Ç –≤—Å–µ –∏—Å–ø—Ä–∞–≤–Ω–æüõ†üî©,–º–∞—à–∏–Ω–∞ —É—Ö–æ–∂–µ–Ω–Ω–∞—è –æ—Ç–ª–∏...</td>\n",
       "      <td>–ê–≤—Ç–æ–º–æ–±–∏–ª–∏</td>\n",
       "      <td>–¢—Ä–∞–Ω—Å–ø–æ—Ä—Ç</td>\n",
       "      <td>1550000.0</td>\n",
       "      <td>–ù–∏–∂–µ–≥–æ—Ä–æ–¥—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å</td>\n",
       "      <td>–ù–∏–∂–Ω–∏–π –ù–æ–≤–≥–æ—Ä–æ–¥</td>\n",
       "      <td>2019-10-14 23:59:31.048375</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4872 rows √ó 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                               title  \\\n",
       "0          11365                                             AirPods   \n",
       "1          11366  –ú–µ—Ö–∞–Ω–∏–∑–º —Å—Ç–µ–∫–ª–æ–æ—á–∏—Å—Ç–∏—Ç–µ–ª—è –ø–µ—Ä–µ–¥–Ω–∏–π Nissan Serena 4   \n",
       "2          11367                                           –ù—à-42–≤-4–ª   \n",
       "3          11368                                                –®–∏–Ω—ã   \n",
       "4          11369                                          –ñ–∏–≤—ã–µ —Ä–∞–∫–∏   \n",
       "...          ...                                                 ...   \n",
       "4867       16232                                   –û—Ñ–∏—Å–Ω–æ–µ –ø–æ–º–µ—â–µ–Ω–∏–µ   \n",
       "4868       16233                           iPhone 8 Plus Silver 25GB   \n",
       "4869       16234                        6-–∫ –∫–≤–∞—Ä—Ç–∏—Ä–∞, 54 –º¬≤, 5/9 —ç—Ç.   \n",
       "4870       16235                        4-–∫ –∫–≤–∞—Ä—Ç–∏—Ä–∞, 60 –º¬≤, 9/9 —ç—Ç.   \n",
       "4871       16236                                   BMW 7 —Å–µ—Ä–∏—è, 1031   \n",
       "\n",
       "                                                                                              description  \\\n",
       "0     Airpods /\\nAirPods 2 C–êMA–Ø –õ–£–ß–®A–Ø –∏ –¢O–ß–ù–ê–Ø –öO–ü–ò–Ø –æ—Ä–∏–≥–∏–Ω–∞–ª–∞ - 5:5, –ü–ûC–õE–î–ù–Ø–Ø M–û–î–ï–õ–¨ - –æ–∫—Ç—è–±—Ä—å 205...   \n",
       "1     –ú–µ—Ö–∞–Ω–∏–∑–º —Å—Ç–µ–∫–ª–æ–æ—á–∏—Å—Ç–∏—Ç–µ–ª—è Nissan Serena 1 C62M GA13DE 1991 –ø–µ—Ä–µ–¥–Ω–∏–π (–±/—É)/\\n/\\n –ú–∞—Ä–∫–∞: Nissan/\\n...   \n",
       "2                                                                                  –Ω–æ–≤–æ–µ. —Ç–µ–ª 89024003360   \n",
       "3     –î–≤–∞ —Å–∫–∞—Ç–∞ DUNLOP –Ø–ø–æ–Ω–∏—è 683 -53-63 —Å–æ—Å—Ç–æ—è–Ω–∏–µ –Ω–æ–≤—ã—Ö –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏—Å—å –æ–¥–∏–Ω —Å–µ–∑–æ–Ω 1 –º–µ—Å—è—Ü–∞ —Ü–µ–Ω–∞ –∑–∞ 2 ...   \n",
       "4     –í –ø—Ä–æ–¥–∞–∂–µ —Ä–∞–∫ –¥–∏–∫–∏–π, —Ä–µ—á–Ω–æ–π./\\n–û–ø—Ç–æ–º –æ—Ç 50 –∫–≥, –≤ —Ä–æ–∑–Ω–∏—Ü—É –æ—Ç 5 –∫–≥./\\n–¶–µ–Ω–∞ –æ–ø—Ç / —Ä–æ–∑–Ω–∏—Ü–∞:/\\n/\\n–º–µ–ª...   \n",
       "...                                                                                                   ...   \n",
       "4867  –°–¥–∞—é—Ç—Å—è –æ—Ñ–∏—Å–Ω–æ–µ –ø–æ–º–µ—â–µ–Ω–∏–µ –ø–æ –£–ª –ö–∞–ª–∏–Ω–∏–Ω–∞ 80. –ü–æ–º–µ—â–µ–Ω–∏—è –Ω–∞—Ö–æ–¥—è—Ç—Å—è –≤ –æ—Ñ–∏—Å–Ω–æ–º —Ü–µ–Ω—Ç—Ä–µ. –í —Å—Ç–æ–∏–º–æ—Å—Ç—å –∞...   \n",
       "4868  –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ä–æ—Å—Ç–µ—Å—Ç –∞–π—Ñ–æ–Ω. –ò–¥–µ–∞–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ, –Ω–∏ —Ä–∞–∑—É –Ω–µ —Ä–µ–º–æ–Ω—Ç–∏—Ä–æ–≤–∞–ª—Å—è –∏ –Ω–µ —Ä–∞–∑–±–∏—Ä–∞–ª—Å—è. /\\n–ö...   \n",
       "4869  _________________________________________________________/\\n /\\n–ü–†–û–°–¢–û–†–ù–ê–Ø –ö–í–ê–†–¢–ò–†–ê –° –•–û–†–û–®–ï–ô –ü–õ...   \n",
       "4870  ¬†/\\n /\\n‚óè –†–∞–±–æ—Ç–∞–µ–º –ë–ï–ó –ü–ï–†–ï–†–´–í–û–í –ò –í–´–•–û–î–ù–´–• —Å 9:00 –¥–æ 12:00./\\n /\\n‚óè –û–±–º–µ–Ω—è–µ–º –∏ –ø—Ä–æ–¥–∞–¥–∏–º –í–∞—à—É –Ω–µ...   \n",
       "4871  –ú–∞—à–∏–Ω–∞ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏ –≤ –æ—Ç–ª–∏—á–Ω–æ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ –≤—Å–µ —Å–∏—Å—Ç–µ–º—ã —Ä–∞–±–æ—Ç–∞—é—Ç –≤—Å–µ –∏—Å–ø—Ä–∞–≤–Ω–æüõ†üî©,–º–∞—à–∏–Ω–∞ —É—Ö–æ–∂–µ–Ω–Ω–∞—è –æ—Ç–ª–∏...   \n",
       "\n",
       "                    subcategory             category      price  \\\n",
       "0                 –ê—É–¥–∏–æ –∏ –≤–∏–¥–µ–æ  –ë—ã—Ç–æ–≤–∞—è —ç–ª–µ–∫—Ç—Ä–æ–Ω–∏–∫–∞     3489.0   \n",
       "1         –ó–∞–ø—á–∞—Å—Ç–∏ –∏ –∞–∫—Å–µ—Å—Å—É–∞—Ä—ã            –¢—Ä–∞–Ω—Å–ø–æ—Ä—Ç     2500.0   \n",
       "2         –ó–∞–ø—á–∞—Å—Ç–∏ –∏ –∞–∫—Å–µ—Å—Å—É–∞—Ä—ã            –¢—Ä–∞–Ω—Å–ø–æ—Ä—Ç     3000.0   \n",
       "3         –ó–∞–ø—á–∞—Å—Ç–∏ –∏ –∞–∫—Å–µ—Å—Å—É–∞—Ä—ã            –¢—Ä–∞–Ω—Å–ø–æ—Ä—Ç     4000.0   \n",
       "4              –ü—Ä–æ–¥—É–∫—Ç—ã –ø–∏—Ç–∞–Ω–∏—è      –î–ª—è –¥–æ–º–∞ –∏ –¥–∞—á–∏      400.0   \n",
       "...                         ...                  ...        ...   \n",
       "4867  –ö–æ–º–º–µ—Ä—á–µ—Å–∫–∞—è –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç—å         –ù–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç—å     9200.0   \n",
       "4868                   –¢–µ–ª–µ—Ñ–æ–Ω—ã  –ë—ã—Ç–æ–≤–∞—è —ç–ª–µ–∫—Ç—Ä–æ–Ω–∏–∫–∞    20000.0   \n",
       "4869                   –ö–≤–∞—Ä—Ç–∏—Ä—ã         –ù–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç—å  3300000.0   \n",
       "4870                   –ö–≤–∞—Ä—Ç–∏—Ä—ã         –ù–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç—å  2300000.0   \n",
       "4871                 –ê–≤—Ç–æ–º–æ–±–∏–ª–∏            –¢—Ä–∞–Ω—Å–ø–æ—Ä—Ç  1550000.0   \n",
       "\n",
       "                     region             city          datetime_submitted  \\\n",
       "0                    –†–æ—Å—Å–∏—è           –ú–æ—Å–∫–≤–∞  2019-10-13 12:43:42.299084   \n",
       "1          –õ–∏–ø–µ—Ü–∫–∞—è –æ–±–ª–∞—Å—Ç—å           –õ–∏–ø–µ—Ü–∫  2019-10-13 12:45:06.628388   \n",
       "2       –ß–µ–ª—è–±–∏–Ω—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å        –ß–µ–ª—è–±–∏–Ω—Å–∫  2019-10-13 12:45:44.957825   \n",
       "3        –†–æ—Å—Ç–æ–≤—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å       –ü—Ä–æ–ª–µ—Ç–∞—Ä—Å–∫  2019-10-13 12:45:45.456624   \n",
       "4                    –†–æ—Å—Å–∏—è           –ú–æ—Å–∫–≤–∞  2019-10-13 12:48:10.767700   \n",
       "...                     ...              ...                         ...   \n",
       "4867                –ß—É–≤–∞—à–∏—è        –ß–µ–±–æ–∫—Å–∞—Ä—ã  2019-10-14 23:57:30.094904   \n",
       "4868              –¢–∞—Ç–∞—Ä—Å—Ç–∞–Ω           –ö–∞–∑–∞–Ω—å  2019-10-14 23:57:50.610616   \n",
       "4869    –°—Ç–∞–≤—Ä–æ–ø–æ–ª—å—Å–∫–∏–π –∫—Ä–∞–π        –ü—è—Ç–∏–≥–æ—Ä—Å–∫  2019-10-14 23:58:02.781579   \n",
       "4870    –°—Ç–∞–≤—Ä–æ–ø–æ–ª—å—Å–∫–∏–π –∫—Ä–∞–π        –ü—è—Ç–∏–≥–æ—Ä—Å–∫  2019-10-14 23:59:01.435691   \n",
       "4871  –ù–∏–∂–µ–≥–æ—Ä–æ–¥—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å  –ù–∏–∂–Ω–∏–π –ù–æ–≤–≥–æ—Ä–æ–¥  2019-10-14 23:59:31.048375   \n",
       "\n",
       "      is_bad  \n",
       "0          0  \n",
       "1          0  \n",
       "2          1  \n",
       "3          1  \n",
       "4          0  \n",
       "...      ...  \n",
       "4867       0  \n",
       "4868       0  \n",
       "4869       0  \n",
       "4870       0  \n",
       "4871       1  \n",
       "\n",
       "[4872 rows x 10 columns]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "24bcb8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'DeepPavlov/distilrubert-base-cased-conversational' # pretrained model from Transformers\n",
    "LOG_DIR = \"./logdir_avito_descr\"    # for training logs and tensorboard visualizations\n",
    "NUM_EPOCHS = 5                         # smth around 2-6 epochs is typically fine when finetuning transformers\n",
    "BATCH_SIZE = 8                        # depends on your available GPU memory (in combination with max seq length)\n",
    "MAX_SEQ_LENGTH = 256                   # depends on your available GPU memory (in combination with batch size)\n",
    "LEARN_RATE = 1e-5                      # learning rate is typically ~1e-5 for transformers\n",
    "ACCUM_STEPS = 4                        # one optimization step for that many backward passes\n",
    "SEED = 31                              # random seed for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "8307395f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassificationDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Wrapper around Torch Dataset to perform text classification\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 texts: List[str],\n",
    "                 labels: List[str] = None,\n",
    "                 label_dict: Mapping[str, int] = None,\n",
    "                 max_seq_length: int = 256,\n",
    "                 model_name: str = 'DeepPavlov/distilrubert-base-cased-conversational'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            texts (List[str]): a list with texts to classify or to train the\n",
    "                classifier on\n",
    "            labels List[str]: a list with classification labels (optional)\n",
    "            label_dict (dict): a dictionary mapping class names to class ids,\n",
    "                to be passed to the validation data (optional)\n",
    "            max_seq_length (int): maximal sequence length in tokens,\n",
    "                texts will be stripped to this length\n",
    "            model_name (str): transformer model name, needed to perform\n",
    "                appropriate tokenization\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.label_dict = label_dict\n",
    "        self.max_seq_length = max_seq_length\n",
    "\n",
    "        if self.label_dict is None and labels is not None:\n",
    "            # {'class1': 0, 'class2': 1, 'class3': 2, ...}\n",
    "            # using this instead of `sklearn.preprocessing.LabelEncoder`\n",
    "            # no easily handle unknown target values\n",
    "            self.label_dict = dict(zip(sorted(set(labels)),\n",
    "                                       range(len(set(labels)))))\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        # suppresses tokenizer warnings\n",
    "        logging.getLogger(\n",
    "            \"transformers.tokenization_utils\").setLevel(logging.FATAL)\n",
    "\n",
    "        # special tokens for transformers\n",
    "        # in the simplest case a [CLS] token is added in the beginning\n",
    "        # and [SEP] token is added in the end of a piece of text\n",
    "        # [CLS] <indexes text tokens> [SEP] .. <[PAD]>\n",
    "        self.sep_vid = self.tokenizer.vocab[\"[SEP]\"]\n",
    "        self.cls_vid = self.tokenizer.vocab[\"[CLS]\"]\n",
    "        self.pad_vid = self.tokenizer.vocab[\"[PAD]\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            int: length of the dataset\n",
    "        \"\"\"\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, index) -> Mapping[str, torch.Tensor]:\n",
    "        \"\"\"Gets element of the dataset\n",
    "\n",
    "        Args:\n",
    "            index (int): index of the element in the dataset\n",
    "        Returns:\n",
    "            Single element by index\n",
    "        \"\"\"\n",
    "\n",
    "        # encoding the text\n",
    "        x = self.texts[index]\n",
    "        x_encoded = self.tokenizer.encode(\n",
    "            x,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_seq_length,\n",
    "            return_tensors=\"pt\",\n",
    "        ).squeeze(0)\n",
    "\n",
    "        # padding short texts\n",
    "        true_seq_length = x_encoded.size(0)\n",
    "        pad_size = self.max_seq_length - true_seq_length\n",
    "        pad_ids = torch.Tensor([self.pad_vid] * pad_size).long()\n",
    "        x_tensor = torch.cat((x_encoded, pad_ids))\n",
    "\n",
    "        # dealing with attention masks - there's a 1 for each input token and\n",
    "        # if the sequence is shorter that `max_seq_length` then the rest is\n",
    "        # padded with zeroes. Attention mask will be passed to the model in\n",
    "        # order to compute attention scores only with input data\n",
    "        # ignoring padding\n",
    "        mask = torch.ones_like(x_encoded, dtype=torch.int8)\n",
    "        mask_pad = torch.zeros_like(pad_ids, dtype=torch.int8)\n",
    "        mask = torch.cat((mask, mask_pad))\n",
    "\n",
    "        output_dict = {\n",
    "            'features' : x_tensor,\n",
    "            'attention_mask' : mask\n",
    "        }\n",
    "\n",
    "        # encoding target\n",
    "        if self.labels is not None:\n",
    "            y = self.labels[index]\n",
    "            y_encoded = torch.Tensor(\n",
    "                [self.label_dict.get(y, -1)]\n",
    "            ).long().squeeze(0)\n",
    "            output_dict[\"targets\"] = y_encoded\n",
    "\n",
    "        return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "782eef35",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TextClassificationDataset(\n",
    "    texts=train_df['description'].values.tolist(),\n",
    "    labels=train_df['is_bad'].values.tolist(),\n",
    "    label_dict=None,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    model_name=MODEL_NAME\n",
    ")\n",
    "\n",
    "valid_dataset = TextClassificationDataset(\n",
    "    texts=valid_df['description'].values.tolist(),\n",
    "    labels=valid_df['is_bad'].values.tolist(),\n",
    "    label_dict=train_dataset.label_dict,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    model_name=MODEL_NAME\n",
    ")\n",
    "\n",
    "test_dataset = TextClassificationDataset(\n",
    "    texts=test_df['description'].values.tolist(),\n",
    "    labels=None,\n",
    "    label_dict=None,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    model_name=MODEL_NAME\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "7994fb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = len(train_dataset.label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "03769946",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_loaders = {\n",
    "    \"train\": DataLoader(dataset=train_dataset,\n",
    "                        batch_size=BATCH_SIZE, \n",
    "                        shuffle=True),\n",
    "    \"valid\": DataLoader(dataset=valid_dataset,\n",
    "                        batch_size=BATCH_SIZE, \n",
    "                        shuffle=False)    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "313e0d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loaders = {\n",
    "    \"test\": DataLoader(\n",
    "            dataset=test_dataset,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            shuffle=False,\n",
    "        )\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "4128d3f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class BertForSequenceClassification(nn.Module):\n",
    "    \"\"\"\n",
    "    Simplified version of the same class by HuggingFace.\n",
    "    See transformers/modeling_distilbert.py in the transformers repository.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, pretrained_model_name: str, num_classes: int = None, dropout: float = 0.3):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            pretrained_model_name (str): HuggingFace model name.\n",
    "                See transformers/modeling_auto.py\n",
    "            num_classes (int): the number of class labels\n",
    "                in the classification task\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        config = AutoConfig.from_pretrained(pretrained_model_name, num_labels=num_classes)\n",
    "\n",
    "        self.model = AutoModel.from_pretrained(pretrained_model_name, config=config)\n",
    "        self.classifier = nn.Linear(config.hidden_size, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, features, attention_mask=None, head_mask=None):\n",
    "        \"\"\"Compute class probabilities for the input sequence.\n",
    "\n",
    "        Args:\n",
    "            features (torch.Tensor): ids of each token,\n",
    "                size ([bs, seq_length]\n",
    "            attention_mask (torch.Tensor): binary tensor, used to select\n",
    "                tokens which are used to compute attention scores\n",
    "                in the self-attention heads, size [bs, seq_length]\n",
    "            head_mask (torch.Tensor): 1.0 in head_mask indicates that\n",
    "                we keep the head, size: [num_heads]\n",
    "                or [num_hidden_layers x num_heads]\n",
    "        Returns:\n",
    "            PyTorch Tensor with predicted class scores\n",
    "        \"\"\"\n",
    "        assert attention_mask is not None, \"attention mask is none\"\n",
    "\n",
    "        # taking BERTModel output\n",
    "        # see https://huggingface.co/transformers/model_doc/bert.html#transformers.BertModel\n",
    "        bert_output = self.model(input_ids=features, attention_mask=attention_mask, head_mask=head_mask)\n",
    "        # we only need the hidden state here and don't need\n",
    "        # transformer output, so index 0\n",
    "        seq_output = bert_output[0]  # (bs, seq_len, dim)\n",
    "        # mean pooling, i.e. getting average representation of all tokens\n",
    "        pooled_output = seq_output.mean(axis=1)  # (bs, dim)\n",
    "        pooled_output = self.dropout(pooled_output)  # (bs, dim)\n",
    "        scores = self.classifier(pooled_output)  # (bs, num_classes)\n",
    "\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953c672c",
   "metadata": {},
   "source": [
    "## Loading pth model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "707b4a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at DeepPavlov/distilrubert-base-cased-conversational were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (model): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (1): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (2): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (3): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (4): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (5): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "model = BertForSequenceClassification(pretrained_model_name=MODEL_NAME, num_classes=2)\n",
    "model.load_state_dict(torch.load(PATH_TO_MODEL))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "21aa90f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (model): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (1): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (2): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (3): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (4): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (5): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "580faa62",
   "metadata": {},
   "outputs": [],
   "source": [
    "runner = SupervisedRunner(model=model, input_key=(\"features\", \"attention_mask\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3015b971",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "3deced65",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'softmax'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8917/1220421257.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m test_pred_scores = np.concatenate(\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;34m[\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"logits\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_loaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m )\n",
      "\u001b[0;32m/tmp/ipykernel_8917/1220421257.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m test_pred_scores = np.concatenate(\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;34m[\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"logits\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_loaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m )\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36msoftmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1816\u001b[0m         \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_softmax_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"softmax\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1817\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1818\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1819\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1820\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'softmax'"
     ]
    }
   ],
   "source": [
    "test_pred_scores = np.concatenate(\n",
    "    [pred[\"logits\"].detach().cpu().numpy() for pred in runner.predict_loader(loader=test_loaders[\"test\"])]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0bc7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "ef2fd50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_score_probs = F.softmax(torch.from_numpy(test_pred_scores), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "cf18b8d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7.9508769e-05, 6.7123474e-05, 9.9641681e-01, ..., 2.0889622e-04,\n",
       "       9.8866665e-05, 9.8717308e-01], dtype=float32)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_score_probs[:, -1].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "90654c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(test_score_probs[:, -1].numpy(), columns=['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "e8c53aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "e8e85db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['perdiction'] = test_score_probs[:, -1].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "15012ca3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>prediction</th>\n",
       "      <th>perdiction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.000080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.996417</td>\n",
       "      <td>0.996417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.998438</td>\n",
       "      <td>0.998438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.001098</td>\n",
       "      <td>0.001098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4867</th>\n",
       "      <td>4867</td>\n",
       "      <td>0.000361</td>\n",
       "      <td>0.000361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4868</th>\n",
       "      <td>4868</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4869</th>\n",
       "      <td>4869</td>\n",
       "      <td>0.000209</td>\n",
       "      <td>0.000209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4870</th>\n",
       "      <td>4870</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>0.000099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4871</th>\n",
       "      <td>4871</td>\n",
       "      <td>0.987173</td>\n",
       "      <td>0.987173</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4872 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index  prediction  perdiction\n",
       "0         0    0.000080    0.000080\n",
       "1         1    0.000067    0.000067\n",
       "2         2    0.996417    0.996417\n",
       "3         3    0.998438    0.998438\n",
       "4         4    0.001098    0.001098\n",
       "...     ...         ...         ...\n",
       "4867   4867    0.000361    0.000361\n",
       "4868   4868    0.000079    0.000079\n",
       "4869   4869    0.000209    0.000209\n",
       "4870   4870    0.000099    0.000099\n",
       "4871   4871    0.987173    0.987173\n",
       "\n",
       "[4872 rows x 3 columns]"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf31fce",
   "metadata": {},
   "source": [
    "## Evaluating metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "47f8bfb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "a28183b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = sorted(train_df['is_bad'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "b6dfaf65",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = [LABELS[i] for i in test_pred_scores.argmax(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "b1dd98a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9350010735567987"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(test_df['is_bad'], test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "c53424fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "c0565e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Sun Aug 28 18:19:06 2022       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:01:00.0  On |                  N/A |\r\n",
      "| N/A   61C    P0    43W /  N/A |   4002MiB /  6144MiB |     94%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A      1671      G   /usr/lib/xorg/Xorg                 64MiB |\r\n",
      "|    0   N/A  N/A      2623      G   /usr/lib/xorg/Xorg                212MiB |\r\n",
      "|    0   N/A  N/A      2803      G   /usr/bin/gnome-shell               42MiB |\r\n",
      "|    0   N/A  N/A      4954      G   /usr/lib/firefox/firefox          298MiB |\r\n",
      "|    0   N/A  N/A      5031      G   telegram-desktop                    1MiB |\r\n",
      "|    0   N/A  N/A      8917      C   /usr/bin/python3                 3359MiB |\r\n",
      "|    0   N/A  N/A     13164      G   ...RendererForSitePerProcess        9MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "ed64d6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9140e5ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734fbffb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d61936",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3592d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
